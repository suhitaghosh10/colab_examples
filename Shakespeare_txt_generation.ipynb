{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespeare_txt_generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqF8v1FYxWQuEtbKd5NDbs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhitaghosh10/colab_examples/blob/master/Shakespeare_txt_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzm_tNBBKOcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPnpP6zqKhrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fc3b7425-6ff4-4c3c-91d8-0c158256898a"
      },
      "source": [
        "path_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXdC8eCVKssh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6abd234-4b93-400e-a376-82eb4048222a"
      },
      "source": [
        "text = open(path_file, 'rb').read().decode(encoding='utf-8')\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG8wjG5zKxbf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "cc8c6ddb-308d-4862-ebbb-42e1787ffd88"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e6QTbv3K1aP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b47d0f46-b7e6-4e4a-ffe2-9ffd8a7f8909"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaKbhSh8LMV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ab749647-e06d-4c24-fe8d-abb7cf458a1d"
      },
      "source": [
        "print(len(vocab))\n",
        "vocab[0:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n', ' ', '!', '$', '&']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejRqJwX8LOOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgbCxmYKLwV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "074e6aa3-7088-4e54-b913-87a2e46389d9"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOBHtOgoPiIT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f2254e4e-fc96-4709-b2df-941a4fb29239"
      },
      "source": [
        "text_as_int.shape\n",
        "print(text_as_int[0:20])\n",
        "print(idx2char[text_as_int[0:20]])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56]\n",
            "['F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'B' 'e' 'f'\n",
            " 'o' 'r']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMM_t45aL5zs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0547b826-0edf-4026-dabc-af0e3338b27a"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "print(examples_per_epoch)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115394,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8KZ94Y7Nzzu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "902b5dc9-c74b-437d-abdc-7806c6a510fd"
      },
      "source": [
        "# Create training examples / targets\n",
        "val_set_index = int(0.2 * len(text))\n",
        "train_set = text_as_int[0:val_set_index]\n",
        "val_set = text_as_int[val_set_index:len(text)]\n",
        "train_char_dataset = tf.data.Dataset.from_tensor_slices(train_set)\n",
        "val_char_dataset = tf.data.Dataset.from_tensor_slices(val_set)\n",
        "for i in train_char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])\n",
        "\n",
        "for i in val_char_dataset.take(10):\n",
        "  print(idx2char[i.numpy()])\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            "r\n",
            " \n",
            "y\n",
            "e\n",
            "a\n",
            "r\n",
            "s\n",
            "\n",
            "\n",
            "H\n",
            "a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2mWfVOkPY_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "b295c933-d04a-4406-f01c-8828d41821d3"
      },
      "source": [
        "train_text_sequences = train_char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "val_text_sequences = val_char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "print(train_text_sequences, val_text_sequences)\n",
        "for item in train_text_sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))\n",
        "\n",
        "print('---------')\n",
        "for item in val_text_sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (101,), types: tf.int64> <BatchDataset shapes: (101,), types: tf.int64>\n",
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
            "---------\n",
            "\"r years\\nHath not yet dived into the world's deceit\\nNor more can you distinguish of a man\\nThan of his \"\n",
            "'outward show; which, God he knows,\\nSeldom or never jumpeth with the heart.\\nThose uncles which you wan'\n",
            "\"t were dangerous;\\nYour grace attended to their sugar'd words,\\nBut look'd not on the poison of their h\"\n",
            "'earts :\\nGod keep you from them, and from such false friends!\\n\\nPRINCE EDWARD:\\nGod keep me from false f'\n",
            "'riends! but they were none.\\n\\nGLOUCESTER:\\nMy lord, the mayor of London comes to greet you.\\n\\nLord Mayor'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0DEittCRgS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1cLejSAVhTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f9ee5a0a-94e7-4c0e-e168-aae4509b0e84"
      },
      "source": [
        "train_dataset = train_text_sequences.map(split_input_target)\n",
        "val_dataset = val_text_sequences.map(split_input_target)\n",
        "train_dataset"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArqBfOuQVwU3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8959fbc9-7863-484e-bc6d-d48962e55e1a"
      },
      "source": [
        "for input_example, target_example in  train_dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGHkCmAwV0x7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJKZF2SDXiy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbUadoHCYB2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "                               \n",
        "                               tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "                               tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "                               tf.keras.layers.Dense(vocab_size)]\n",
        "      \n",
        "                              )\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm0TUOEtZZyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR8W1GrVZlu1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a37c83ef-2fe1-4daa-bd1f-de88027bc1aa"
      },
      "source": [
        "for input_example_batch, target_example_batch in train_dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHbEVeqJamWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J05whAbVazdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "0fc7ef18-e395-40a4-e7a2-257fc29c3254"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"hat do appear,\\nTheir needless vouches? Custom calls me to't:\\nWhat custom wills, in all things should\"\n",
            "\n",
            "Next Char Predictions: \n",
            " \"aheWdqreX:P;hSPkawZSvYDBRRGEOEnZNAWqqVTBxB.,'aI'BvsWhtlQAAda&'DDiJlJm&widtGH?Ec\\nhMGFJbHKEVC,iNpRF-s-\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiC3abdVa2S1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "37e4dab9-8ea9-46ff-c60b-1b885b03cd17"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1754785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYqpGteda7ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_loss',\n",
        "                                           save_best_only=True,\n",
        "                                           verbose=1,\n",
        "                                           mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGyaLOWRa-Xm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "96657f3a-bb1e-4a0e-8d3b-354949c8aa97"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "\n",
        "history = model.fit(train_dataset, epochs=10, callbacks=[model_checkpoint], validation_data=val_dataset)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 3.6421\n",
            "Epoch 00001: val_loss improved from inf to 3.05007, saving model to model.h5\n",
            "34/34 [==============================] - 6s 171ms/step - loss: 3.6421 - val_loss: 3.0501\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 2.7336\n",
            "Epoch 00002: val_loss improved from 3.05007 to 2.54596, saving model to model.h5\n",
            "34/34 [==============================] - 6s 162ms/step - loss: 2.7336 - val_loss: 2.5460\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 2.3778\n",
            "Epoch 00003: val_loss improved from 2.54596 to 2.38631, saving model to model.h5\n",
            "34/34 [==============================] - 6s 162ms/step - loss: 2.3778 - val_loss: 2.3863\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 2.2406\n",
            "Epoch 00004: val_loss improved from 2.38631 to 2.31056, saving model to model.h5\n",
            "34/34 [==============================] - 5s 162ms/step - loss: 2.2406 - val_loss: 2.3106\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 2.1296\n",
            "Epoch 00005: val_loss improved from 2.31056 to 2.24577, saving model to model.h5\n",
            "34/34 [==============================] - 5s 162ms/step - loss: 2.1296 - val_loss: 2.2458\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 2.0227\n",
            "Epoch 00006: val_loss improved from 2.24577 to 2.18229, saving model to model.h5\n",
            "34/34 [==============================] - 5s 162ms/step - loss: 2.0227 - val_loss: 2.1823\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 1.9220\n",
            "Epoch 00007: val_loss improved from 2.18229 to 2.12581, saving model to model.h5\n",
            "34/34 [==============================] - 5s 161ms/step - loss: 1.9220 - val_loss: 2.1258\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 1.8389\n",
            "Epoch 00008: val_loss improved from 2.12581 to 2.07517, saving model to model.h5\n",
            "34/34 [==============================] - 6s 162ms/step - loss: 1.8389 - val_loss: 2.0752\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 1.7628\n",
            "Epoch 00009: val_loss improved from 2.07517 to 2.03452, saving model to model.h5\n",
            "34/34 [==============================] - 6s 162ms/step - loss: 1.7628 - val_loss: 2.0345\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - ETA: 0s - loss: 1.6972\n",
            "Epoch 00010: val_loss improved from 2.03452 to 1.99356, saving model to model.h5\n",
            "34/34 [==============================] - 6s 162ms/step - loss: 1.6972 - val_loss: 1.9936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KhT58Ezg_nC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### prediction\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights('model.h5')\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMj0FQ2GhFIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  print(input_eval)\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  print(input_eval.shape)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o6kPTuche7j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "outputId": "514b64e4-d7cb-4549-eb7e-4dc7a31b3720"
      },
      "source": [
        "print(generate_text(model, start_string=u\"Rome \"))\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[30, 53, 51, 43, 1]\n",
            "(1, 5)\n",
            "Rome have y ah hears,\n",
            "Make his lise my &Akn'sbay,\n",
            "detare it in a stond I, why hest turbeld goods tostermen ton uit teir bet;\n",
            "And tround us'd heave hour fooron feart, sur, we partion. I would bus nat: I hear heard\n",
            "I was near that ye ervy ather the gods, home\n",
            "That I lafe come, lettor: whence tham I soat fooret.\n",
            "Nouthing the searte this were on all todshy.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "You eeat; whis is with,\n",
            "As my bard maven I'st a a place fos tether\n",
            "To you are of.\n",
            "\n",
            "KICSICINIUS:\n",
            "I way not to have crile:\n",
            "Which he sarvint swarm;\n",
            "To of diving conds; but cullopaict.\n",
            "\n",
            "QULER:\n",
            "And he hadbs,\n",
            "Alvain I he it.\n",
            "\n",
            "BRUTUS:\n",
            "Othy? I fouxt mear,\n",
            "As nevore!\n",
            "And your vertence as of your goge,\n",
            "And theil uc:\n",
            "Thang as thee flear deditidion'd to tuses thag,?\n",
            "If thou ard him\n",
            "ond dive with as mearing atsiness,\n",
            "And het me rapies, brithrite, then mest upon to since in thoute than wo bejour drear: thou shild,\n",
            "And math reat, word, had, forkect\n",
            "Thou be qutered solf face stild of I heart,\n",
            "QUS:\n",
            "Gol, of the defores\n",
            "What her chatterake, I\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq1F4HT9him4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###using gradient tape\n",
        "\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  )\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUDy_UQ1Kw8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMa36aErJs9H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "906ceb4c-6c78-41d7-e260-d96d681493fe"
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Epoch 1 Batch 0 Loss 4.175806999206543\n",
            "Epoch 1 Batch 100 Loss 2.3720622062683105\n",
            "Epoch 1 Loss 2.1846\n",
            "Time taken for 1 epoch 13.01163125038147 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.1800589561462402\n",
            "Epoch 2 Batch 100 Loss 1.9297655820846558\n",
            "Epoch 2 Loss 1.9009\n",
            "Time taken for 1 epoch 11.82506799697876 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7756026983261108\n",
            "Epoch 3 Batch 100 Loss 1.7015266418457031\n",
            "Epoch 3 Loss 1.5853\n",
            "Time taken for 1 epoch 11.859676837921143 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6328121423721313\n",
            "Epoch 4 Batch 100 Loss 1.5122989416122437\n",
            "Epoch 4 Loss 1.5188\n",
            "Time taken for 1 epoch 11.969431161880493 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4673004150390625\n",
            "Epoch 5 Batch 100 Loss 1.4484620094299316\n",
            "Epoch 5 Loss 1.4903\n",
            "Time taken for 1 epoch 11.974555492401123 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3874232769012451\n",
            "Epoch 6 Batch 100 Loss 1.41978120803833\n",
            "Epoch 6 Loss 1.3930\n",
            "Time taken for 1 epoch 12.035968542098999 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3656483888626099\n",
            "Epoch 7 Batch 100 Loss 1.3535572290420532\n",
            "Epoch 7 Loss 1.3430\n",
            "Time taken for 1 epoch 12.029869318008423 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2754226922988892\n",
            "Epoch 8 Batch 100 Loss 1.3319082260131836\n",
            "Epoch 8 Loss 1.3052\n",
            "Time taken for 1 epoch 12.289788484573364 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2421799898147583\n",
            "Epoch 9 Batch 100 Loss 1.3126498460769653\n",
            "Epoch 9 Loss 1.2917\n",
            "Time taken for 1 epoch 12.060323476791382 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.206566572189331\n",
            "Epoch 10 Batch 100 Loss 1.2568976879119873\n",
            "Epoch 10 Loss 1.2534\n",
            "Time taken for 1 epoch 12.198282718658447 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV1Os2izKIVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}